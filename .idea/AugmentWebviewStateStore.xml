<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="AugmentWebviewStateStore">
    <option name="stateMap">
      <map>
        <entry key="CHAT_STATE" value="{&quot;currentConversationId&quot;:&quot;3460035d-76c8-4f99-9e3a-3c4e768d5fac&quot;,&quot;conversations&quot;:{&quot;3460035d-76c8-4f99-9e3a-3c4e768d5fac&quot;:{&quot;id&quot;:&quot;3460035d-76c8-4f99-9e3a-3c4e768d5fac&quot;,&quot;createdAtIso&quot;:&quot;2025-08-04T15:42:46.576Z&quot;,&quot;lastInteractedAtIso&quot;:&quot;2025-08-04T15:58:13.455Z&quot;,&quot;chatHistory&quot;:[{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;11f89855-a615-4d06-8a3d-61f0db34383c&quot;,&quot;request_message&quot;:&quot;Below is a concise but complete blueprint for an **Apify Actor** that takes a single *search query* (plus an optional region/language override) and returns **title + URL + publication date + source + full text + list of working image URLs** for every Google News result it can reach.\n\n---\n\n## 0 . Why RSS + a two-stage crawl still works\n\n* Google News continues to expose every search or topic through RSS endpoints such as\n  `https://news.google.com/rss/search?q=&lt;QUERY&gt;&amp;hl=&lt;LANG&gt;&amp;gl=&lt;COUNTRY&gt;&amp;ceid=&lt;COUNTRY&gt;:&lt;LANG&gt;` ([newscatcherapi.com][1]).\n* Region/language are controlled by `hl`, `gl`, `ceid`; omit them and Google falls back to `en-US / US`([docs.feedly.com][2]).\n* RSS items carry only the headline &amp; link, so we must open each article ourselves to collect images and full text.\n\n---\n\n## 1 . Define the Actor’s input schema (`INPUT_SCHEMA.json`)\n\n```json\n{\n  \&quot;title\&quot;: \&quot;Google-News-Full-Scraper\&quot;,\n  \&quot;type\&quot;: \&quot;object\&quot;,\n  \&quot;properties\&quot;: {\n    \&quot;query\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;title\&quot;: \&quot;Google News search query\&quot; },\n    \&quot;region\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;default\&quot;: \&quot;US\&quot;, \&quot;title\&quot;: \&quot;Country code (gl)\&quot; },\n    \&quot;language\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;default\&quot;: \&quot;en-US\&quot;, \&quot;title\&quot;: \&quot;Language (hl)\&quot; },\n    \&quot;maxItems\&quot;: { \&quot;type\&quot;: \&quot;integer\&quot;, \&quot;default\&quot;: 0, \&quot;minimum\&quot;: 0, \&quot;title\&quot;: \&quot;0 = all available\&quot; },\n    \&quot;dateFrom\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;format\&quot;: \&quot;date\&quot;, \&quot;title\&quot;: \&quot;yyyy-MM-dd (optional)\&quot; },\n    \&quot;dateTo\&quot;:   { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;format\&quot;: \&quot;date\&quot;, \&quot;title\&quot;: \&quot;yyyy-MM-dd (optional)\&quot; },\n    \&quot;useBrowser\&quot;: { \&quot;type\&quot;: \&quot;boolean\&quot;, \&quot;default\&quot;: false, \&quot;title\&quot;: \&quot;Render JS with Playwright if needed\&quot; }\n  },\n  \&quot;required\&quot;: [\&quot;query\&quot;]\n}\n```\n\n---\n\n## 2 . Stage A – collect every RSS item\n\n| Step | What to do                                                                                                                                                                                                                                                                                                                                               | Key points                                                       |\n| ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |\n| A-1  | **Build the first feed URL** with the user’s `query`, `hl`, `gl`, `ceid`.                                                                                                                                                                                                                                                                                | Encode the query with `encodeURIComponent`.                      |\n| A-2  | **Fetch the feed** with `utils.requestAsBrowser({ url, method: 'GET', ... })`.                                                                                                                                                                                                                                                                           | Fast—no browser.                                                 |\n| A-3  | **Parse XML** via `fast-xml-parser`; push items into an in-memory Set keyed by GUID to de-dup. Each item gives us `{ title, link, pubDate, source }`.                                                                                                                                                                                                    | If `maxItems &gt; 0` stop when reached.                             |\n| A-4  | **If more items requested than Google’s \\~100-result cap**, slice the date range:&lt;br&gt;‣ Start with `dateTo` (or today) and walk backwards one day at a time.&lt;br&gt;‣ Append `before:` &amp; `after:` operators to the *query* (e.g. `\&quot;AI after:2025-05-10 before:2025-05-11\&quot;`).&lt;br&gt;‣ Repeat A-2 → A-3 until the Set size ≥ `maxItems` or we run out of days.&lt;br&gt; | One HTTP request per day is still faster than headless browsing. |\n\n&gt; **Tip:** store `lastDateChecked` in the Key-Value-Store so the actor can resume if it is restarted.\n\n---\n\n## 3 . Stage B – crawl every article URL\n\n### 3.1  Queue preparation\n\n* For each RSS item, extract the real URL:\n  *If the link already points off `news.google.com`, use it.*\n  *If it looks like `https://news.google.com/rss/articles/...&amp;url=&lt;ENCODED&gt;` → `decodeURIComponent(url)`.*\n* Push that URL into an **Apify RequestQueue** with `{ userData: { metaFromRssItem } }`.\n\n### 3.2  Pick the right crawler\n\n| If `useBrowser`   | Use                 | When                                              |\n| ----------------- | ------------------- | ------------------------------------------------- |\n| `false` (default) | `CheerioCrawler`    | 90 % of news sites deliver pure HTML.             |\n| `true`            | `PlaywrightCrawler` | For pay-walled or JS-rendered pages; higher cost. |\n\nCrawler options:\n\n```js\n{\n  maxConcurrency: 10,\n  maxRequestRetries: 2,\n  requestHandlerTimeoutSecs: 60\n}\n```\n\n### 3.3  In the `requestHandler`\n\n1. **Grab HTML** (`context.response.body`).\n2. **Parse** with **Unfluff** (Node) or **newspaper3k** (Python) to get:\n\n   * `title, text, author, date, image, videos, links, tags, lang`\n3. **Collect images**\n\n   * Combine `unfluff.image`, every `&lt;meta property=\&quot;og:image\&quot;&gt;`, and the first `&lt;article&gt; img`.\n   * **Deduplicate** the list.\n4. **Validate images**\n\n   ```js\n   const workingImages = [];\n   for (const imgUrl of images) {\n     try {\n       const { statusCode } = await utils.requestAsBrowser({ url: imgUrl, method: 'HEAD', timeoutSecs: 15, followRedirect: true });\n       if (statusCode &gt;= 200 &amp;&amp; statusCode &lt; 400) workingImages.push(imgUrl);\n     } catch { /* ignore */ }\n   }\n   ```\n5. **Push final record** to the default Dataset:\n\n   ```js\n   await Dataset.pushData({\n     query: input.query,\n     title,\n     url: page.url(),\n     source: meta.source,\n     publishedAt: meta.pubDate || date,\n     author,\n     text,\n     images: workingImages\n   });\n   ```\n\n---\n\n## 4 . Putting it together (`main.js` skeleton)\n\n```js\nimport { Actor, utils, Dataset, KeyValueStore, RequestQueue, CheerioCrawler } from 'apify';\nimport unfluff from 'unfluff';\nimport { parse } from 'fast-xml-parser';\n\nawait Actor.init();\nconst input = await Actor.getInput();\nconst { query, region = 'US', language = 'en-US', maxItems = 0, dateFrom, dateTo, useBrowser } = input;\n\n// ---------- Stage A ----------\nconst articles = new Map();   // guid → rssItem\n\nasync function fetchFeed(feedUrl) {\n  const { body } = await utils.requestAsBrowser({ url: feedUrl, timeoutSecs: 30 });\n  const xml = parse(body, { ignoreAttributes: false });\n  for (const item of xml.rss.channel.item ?? []) {\n    if (!articles.has(item.guid)) articles.set(item.guid, item);\n    if (maxItems &amp;&amp; articles.size &gt;= maxItems) return;\n  }\n}\n\nfunction buildFeed(q) {\n  const params = new URLSearchParams({\n    q: q,\n    hl: language,\n    gl: region,\n    ceid: `${region}:${language.split('-')[0]}`\n  });\n  return `https://news.google.com/rss/search?${params.toString()}`;\n}\n\n// Initial fetch or date slicing\nawait fetchFeed(buildFeed(query));\nif (maxItems &amp;&amp; articles.size &lt; maxItems) {\n  // walk back one day at a time between dateFrom / dateTo (or today)\n  // append after: / before: to query, call fetchFeed() repeatedly\n}\n\n// ---------- Stage B ----------\nconst rq = await RequestQueue.open();\nfor (const item of articles.values()) {\n  let url = item.link;\n  const u = new URL(url);\n  if (u.hostname === 'news.google.com' &amp;&amp; u.searchParams.has('url')) {\n    url = decodeURIComponent(u.searchParams.get('url'));\n  }\n  await rq.addRequest({ url, userData: item });\n}\n\nconst crawler = new CheerioCrawler({\n  requestQueue: rq,\n  maxConcurrency: 10,\n  requestHandler: async ({ request, $, body }) =&gt; {\n    const data = unfluff(body);\n    // ... image logic from step 3.3 ...\n    await Dataset.pushData(/* final record object */);\n  }\n});\nawait crawler.run();\n\nawait Actor.exit();\n```\n\nConvert to **TypeScript** if you want stricter typing; for **Python** replicate the same flow with `apify` + `aiohttp` + `python-dateutil` + `extruct`/`newspaper3k`.\n\n---\n\n## 5 . Testing &amp; deployment checklist\n\n1. `apify run` locally with a narrow query, no date slicing – verify dataset rows.\n2. Flip `useBrowser=true` on a JS-heavy site (e.g. *washingtonpost.com*) and ensure Playwright works.\n3. Stress-test with `maxItems=1000` – watch CPU &amp; memory; raise `APIFY_MEMORY_MBYTES` if needed.\n4. Set **Actor permissions**: allow `proxy = auto` only if you hit geo-based blocks.\n5. Push to the Apify Store; add *README* documenting the input fields and output schema.\n\n---\n\n### Extensions &amp; hardening (optional but recommended)\n\n* **Rate-limit Google**: add a short `await Actor.utils.sleep(200)` between RSS fetches.\n* **Robust date slicing**: binary-search the date range instead of daily stepping for very large queries.\n* **Retry/timeout hooks**: log failing article URLs to a separate Key-Value-Store key for audit.\n* **Unit tests**: mock an RSS XML and one article HTML to test parsing logic without network.\n* **Structured output**: emit a `.csv` or `.xlsx` in `key_value_stores/default/OUTPUT` for users who prefer spreadsheets.\n\n---\n\nWith this roadmap you can stand up a lean, reliable Google News → full-article scraper inside Apify in just a few hours, without dragging along the complexity of the full-featured actors you studied.\n\n[1]: https://www.newscatcherapi.com/blog/google-news-rss-search-parameters-the-missing-documentaiton?utm_source=chatgpt.com \&quot;Google News RSS Search Parameters: The Missing Docs\&quot;\n[2]: https://docs.feedly.com/article/375-what-are-some-of-the-advanced-keyword-alerts-google-news-search-parameters?utm_source=chatgpt.com \&quot;What are some of the advanced Google News search ...\&quot;\n\n\n### Short answer\n\n**Yes — use a proxy, at least for anything beyond quick, low-volume tests.**\nGoogle and many publisher sites rate-limit or geo-gate aggressive scrapers; rotating proxies keep you off their radar and let you choose the country that Google News should think you’re coming from. Below are the situations, choices and code snippets you’ll need.\n\n---\n\n## 1 . When you can get away **without** a proxy\n\n| Scenario                                                      | Risk of blocks | Notes                                                                                                             |\n| ------------------------------------------------------------- | -------------- | ----------------------------------------------------------------------------------------------------------------- |\n| ≤ a dozen RSS requests per hour and &lt; 100 article fetches     | **Low**        | Google News RSS is relatively tolerant, and most small/medium newsrooms don’t throttle HTML fetches that lightly. |\n| One-off debug runs from your laptop or the Apify “Run” button | **Low**        | Fine for dev, but the shared IP of the Apify build machine is still visible to Google.                            |\n\nIf that’s all you’ll ever do, skip proxies and keep the code simpler. Otherwise, read on.\n\n---\n\n## 2 . Why most production runs **should** use proxies\n\n| Phase                                        | Typical failure without proxy                                                    | Recommended proxy group                                                            |\n| -------------------------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n| **Stage A – RSS calls to `news.google.com`** | `429 Too many requests` or empty feed after \\~100–200 calls in a burst           | `GOOGLE_SERP` (rotation &amp; Google-safe) ([docs.apify.com][1])                       |\n| **Stage B – Fetching article pages**         | Individual publishers return paywall / captcha after 5-30 rapid hits from one IP | `RESIDENTIAL` (safest) or a datacenter group if cost matters ([docs.apify.com][2]) |\n\n* Apify’s own Google-News actors actually **require** a proxy input for this reason. ([Apify][3])\n* All Apify accounts (even free) already include the `GOOGLE_SERP`, `RESIDENTIAL`, and a shared datacenter pool, so you don’t need an extra provider. ([docs.apify.com][4])\n\n---\n\n## 3 . Minimal proxy config you can copy-paste\n\n```js\n// Stage A – Google News RSS\nconst googleProxy = await Actor.createProxyConfiguration({\n    groups: ['GOOGLE_SERP'],          // rotation tuned for Google\n    countryCode: input.region || 'US' // honours user’s region choice\n});\n\n// Stage B – Article pages\nconst articleProxy = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],          // least likely to be blocked\n    countryCode: input.region || 'US'\n});\n```\n\nPass `googleProxy` to the helper that fetches RSS, and `articleProxy` to the crawler that opens every publisher URL.\nDon’t mix: `GOOGLE_SERP` only works for Google domains and will fail on `nytimes.com` or `bbc.co.uk`. ([docs.apify.com][1])\n\n---\n\n## 4 . Tuning tips\n\n| Tip                                                                                                                                     | Why / how                                                                |\n| --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |\n| **Throttle** your RSS loop to \\~3 req/s and article crawler to \\~5 req/s per proxy IP.                                                  | Staying under human-like speed massively reduces bans.                   |\n| **Tie proxies to sessions** (`useSessionPool: true`, `maxUsageCount: 5`).                                                               | Lets you reuse cookies while still rotating IPs before reputation tanks. |\n| **Country targeting:** The proxy’s `countryCode`, not the Google News URL, determines the IP’s location. Leave it `US` as your default. |                                                                          |\n| **Fallback strategy:** If an article 403s on datacenter proxy, retry once with `RESIDENTIAL`.                                           |                                                                          |\n| **Cost control:** Datacenter traffic is cheapest; start there and only promote domains that block you to residential.                   |                                                                          |\n\n---\n\n## 5 . What happens if you ignore proxies?\n\n* Google can start returning a truncated, “filtered for spam” feed, or simply respond with HTTP 429 after a few hundred unauthenticated hits from the same IP range.\n* Popular publishers (e.g., *Washington Post*, *Bloomberg*) detect scraping patterns and either throw paywall splash pages or block the IP entirely.\n* Your Apify actor might work in dev, then silently die during a bigger run and leave you with partial data.\n\nAll of that costs more engineering time than a few proxy credits.\n\n---\n\n### Bottom line\n\n* **Dev/QA:** feel free to run without a proxy.\n* **Anything you plan to schedule or share:** wire in `GOOGLE_SERP` for the Google step and a datacenter → residential fallback for article steps.\n\nThat two-line `createProxyConfiguration` block is the cheapest insurance you can buy for a scraper.\n\n[1]: https://docs.apify.com/platform/proxy/google-serp-proxy \&quot;Google SERP proxy | Platform | Apify Documentation\&quot;\n[2]: https://docs.apify.com/platform/proxy \&quot;Proxy | Platform | Apify Documentation\&quot;\n[3]: https://apify.com/epctex/google-news-scraper \&quot;Google News Scraper · Apify\&quot;\n[4]: https://docs.apify.com/academy/expert-scraping-with-apify/solutions/rotating-proxies \&quot;VI - Rotating proxies/sessions | Academy | Apify Documentation\&quot;\n\n---------\nCan you create a task list based on that descriptions, taking into account Language - js + local setup, tests, deployement, etc etc etc - I need a fully working project as a result&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Below is a concise but complete blueprint for an **Apify Actor** that takes a single *search query* (plus an optional region/language override) and returns **title + URL + publication date + source + full text + list of working image URLs** for every Google News result it can reach.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 0 . Why RSS + a two-stage crawl still works&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* Google News continues to expose every search or topic through RSS endpoints such as&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  `https://news.google.com/rss/search?q=&lt;QUERY&gt;&amp;hl=&lt;LANG&gt;&amp;gl=&lt;COUNTRY&gt;&amp;ceid=&lt;COUNTRY&gt;:&lt;LANG&gt;` ([newscatcherapi.com][1]).&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* Region/language are controlled by `hl`, `gl`, `ceid`; omit them and Google falls back to `en-US / US`([docs.feedly.com][2]).&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* RSS items carry only the headline &amp; link, so we must open each article ourselves to collect images and full text.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 1 . Define the Actor’s input schema (`INPUT_SCHEMA.json`)&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```json&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;{&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  \&quot;title\&quot;: \&quot;Google-News-Full-Scraper\&quot;,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  \&quot;type\&quot;: \&quot;object\&quot;,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  \&quot;properties\&quot;: {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    \&quot;query\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;title\&quot;: \&quot;Google News search query\&quot; },&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    \&quot;region\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;default\&quot;: \&quot;US\&quot;, \&quot;title\&quot;: \&quot;Country code (gl)\&quot; },&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    \&quot;language\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;default\&quot;: \&quot;en-US\&quot;, \&quot;title\&quot;: \&quot;Language (hl)\&quot; },&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    \&quot;maxItems\&quot;: { \&quot;type\&quot;: \&quot;integer\&quot;, \&quot;default\&quot;: 0, \&quot;minimum\&quot;: 0, \&quot;title\&quot;: \&quot;0 = all available\&quot; },&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    \&quot;dateFrom\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;format\&quot;: \&quot;date\&quot;, \&quot;title\&quot;: \&quot;yyyy-MM-dd (optional)\&quot; },&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    \&quot;dateTo\&quot;:   { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;format\&quot;: \&quot;date\&quot;, \&quot;title\&quot;: \&quot;yyyy-MM-dd (optional)\&quot; },&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    \&quot;useBrowser\&quot;: { \&quot;type\&quot;: \&quot;boolean\&quot;, \&quot;default\&quot;: false, \&quot;title\&quot;: \&quot;Render JS with Playwright if needed\&quot; }&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  },&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  \&quot;required\&quot;: [\&quot;query\&quot;]&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;}&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 2 . Stage A – collect every RSS item&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| Step | What to do                                                                                                                                                                                                                                                                                                                                               | Key points                                                       |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| A-1  | **Build the first feed URL** with the user’s `query`, `hl`, `gl`, `ceid`.                                                                                                                                                                                                                                                                                | Encode the query with `encodeURIComponent`.                      |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| A-2  | **Fetch the feed** with `utils.requestAsBrowser({ url, method: 'GET', ... })`.                                                                                                                                                                                                                                                                           | Fast—no browser.                                                 |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| A-3  | **Parse XML** via `fast-xml-parser`; push items into an in-memory Set keyed by GUID to de-dup. Each item gives us `{ title, link, pubDate, source }`.                                                                                                                                                                                                    | If `maxItems &gt; 0` stop when reached.                             |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| A-4  | **If more items requested than Google’s \\~100-result cap**, slice the date range:&lt;br&gt;‣ Start with `dateTo` (or today) and walk backwards one day at a time.&lt;br&gt;‣ Append `before:` &amp; `after:` operators to the *query* (e.g. `\&quot;AI after:2025-05-10 before:2025-05-11\&quot;`).&lt;br&gt;‣ Repeat A-2 → A-3 until the Set size ≥ `maxItems` or we run out of days.&lt;br&gt; | One HTTP request per day is still faster than headless browsing. |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;&gt; **Tip:** store `lastDateChecked` in the Key-Value-Store so the actor can resume if it is restarted.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 3 . Stage B – crawl every article URL&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;### 3.1  Queue preparation&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* For each RSS item, extract the real URL:&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  *If the link already points off `news.google.com`, use it.*&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  *If it looks like `https://news.google.com/rss/articles/...&amp;url=&lt;ENCODED&gt;` → `decodeURIComponent(url)`.*&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* Push that URL into an **Apify RequestQueue** with `{ userData: { metaFromRssItem } }`.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;### 3.2  Pick the right crawler&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| If `useBrowser`   | Use                 | When                                              |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| ----------------- | ------------------- | ------------------------------------------------- |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| `false` (default) | `CheerioCrawler`    | 90 % of news sites deliver pure HTML.             |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| `true`            | `PlaywrightCrawler` | For pay-walled or JS-rendered pages; higher cost. |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Crawler options:&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```js&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;{&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  maxConcurrency: 10,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  maxRequestRetries: 2,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  requestHandlerTimeoutSecs: 60&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;}&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;### 3.3  In the `requestHandler`&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;1. **Grab HTML** (`context.response.body`).&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;2. **Parse** with **Unfluff** (Node) or **newspaper3k** (Python) to get:&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   * `title, text, author, date, image, videos, links, tags, lang`&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;3. **Collect images**&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   * Combine `unfluff.image`, every `&lt;meta property=\&quot;og:image\&quot;&gt;`, and the first `&lt;article&gt; img`.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   * **Deduplicate** the list.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;4. **Validate images**&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   ```js&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   const workingImages = [];&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   for (const imgUrl of images) {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     try {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;       const { statusCode } = await utils.requestAsBrowser({ url: imgUrl, method: 'HEAD', timeoutSecs: 15, followRedirect: true });&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;       if (statusCode &gt;= 200 &amp;&amp; statusCode &lt; 400) workingImages.push(imgUrl);&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     } catch { /* ignore */ }&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   }&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   ```&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;5. **Push final record** to the default Dataset:&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   ```js&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   await Dataset.pushData({&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     query: input.query,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     title,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     url: page.url(),&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     source: meta.source,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     publishedAt: meta.pubDate || date,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     author,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     text,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;     images: workingImages&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   });&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;   ```&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 4 . Putting it together (`main.js` skeleton)&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```js&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;import { Actor, utils, Dataset, KeyValueStore, RequestQueue, CheerioCrawler } from 'apify';&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;import unfluff from 'unfluff';&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;import { parse } from 'fast-xml-parser';&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;await Actor.init();&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;const input = await Actor.getInput();&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;const { query, region = 'US', language = 'en-US', maxItems = 0, dateFrom, dateTo, useBrowser } = input;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;// ---------- Stage A ----------&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;const articles = new Map();   // guid → rssItem&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;async function fetchFeed(feedUrl) {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  const { body } = await utils.requestAsBrowser({ url: feedUrl, timeoutSecs: 30 });&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  const xml = parse(body, { ignoreAttributes: false });&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  for (const item of xml.rss.channel.item ?? []) {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    if (!articles.has(item.guid)) articles.set(item.guid, item);&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    if (maxItems &amp;&amp; articles.size &gt;= maxItems) return;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  }&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;}&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;function buildFeed(q) {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  const params = new URLSearchParams({&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    q: q,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    hl: language,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    gl: region,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    ceid: `${region}:${language.split('-')[0]}`&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  });&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  return `https://news.google.com/rss/search?${params.toString()}`;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;}&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;// Initial fetch or date slicing&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;await fetchFeed(buildFeed(query));&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;if (maxItems &amp;&amp; articles.size &lt; maxItems) {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  // walk back one day at a time between dateFrom / dateTo (or today)&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  // append after: / before: to query, call fetchFeed() repeatedly&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;}&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;// ---------- Stage B ----------&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;const rq = await RequestQueue.open();&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;for (const item of articles.values()) {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  let url = item.link;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  const u = new URL(url);&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  if (u.hostname === 'news.google.com' &amp;&amp; u.searchParams.has('url')) {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    url = decodeURIComponent(u.searchParams.get('url'));&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  }&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  await rq.addRequest({ url, userData: item });&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;}&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;const crawler = new CheerioCrawler({&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  requestQueue: rq,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  maxConcurrency: 10,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  requestHandler: async ({ request, $, body }) =&gt; {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    const data = unfluff(body);&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    // ... image logic from step 3.3 ...&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    await Dataset.pushData(/* final record object */);&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  }&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;});&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;await crawler.run();&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;await Actor.exit();&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Convert to **TypeScript** if you want stricter typing; for **Python** replicate the same flow with `apify` + `aiohttp` + `python-dateutil` + `extruct`/`newspaper3k`.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 5 . Testing &amp; deployment checklist&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;1. `apify run` locally with a narrow query, no date slicing – verify dataset rows.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;2. Flip `useBrowser=true` on a JS-heavy site (e.g. *washingtonpost.com*) and ensure Playwright works.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;3. Stress-test with `maxItems=1000` – watch CPU &amp; memory; raise `APIFY_MEMORY_MBYTES` if needed.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;4. Set **Actor permissions**: allow `proxy = auto` only if you hit geo-based blocks.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;5. Push to the Apify Store; add *README* documenting the input fields and output schema.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;### Extensions &amp; hardening (optional but recommended)&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* **Rate-limit Google**: add a short `await Actor.utils.sleep(200)` between RSS fetches.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* **Robust date slicing**: binary-search the date range instead of daily stepping for very large queries.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* **Retry/timeout hooks**: log failing article URLs to a separate Key-Value-Store key for audit.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* **Unit tests**: mock an RSS XML and one article HTML to test parsing logic without network.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* **Structured output**: emit a `.csv` or `.xlsx` in `key_value_stores/default/OUTPUT` for users who prefer spreadsheets.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;With this roadmap you can stand up a lean, reliable Google News → full-article scraper inside Apify in just a few hours, without dragging along the complexity of the full-featured actors you studied.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;[1]: https://www.newscatcherapi.com/blog/google-news-rss-search-parameters-the-missing-documentaiton?utm_source=chatgpt.com \&quot;Google News RSS Search Parameters: The Missing Docs\&quot;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;[2]: https://docs.feedly.com/article/375-what-are-some-of-the-advanced-keyword-alerts-google-news-search-parameters?utm_source=chatgpt.com \&quot;What are some of the advanced Google News search ...\&quot;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;### Short answer&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;**Yes — use a proxy, at least for anything beyond quick, low-volume tests.**&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Google and many publisher sites rate-limit or geo-gate aggressive scrapers; rotating proxies keep you off their radar and let you choose the country that Google News should think you’re coming from. Below are the situations, choices and code snippets you’ll need.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 1 . When you can get away **without** a proxy&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| Scenario                                                      | Risk of blocks | Notes                                                                                                             |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| ------------------------------------------------------------- | -------------- | ----------------------------------------------------------------------------------------------------------------- |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| ≤ a dozen RSS requests per hour and &lt; 100 article fetches     | **Low**        | Google News RSS is relatively tolerant, and most small/medium newsrooms don’t throttle HTML fetches that lightly. |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| One-off debug runs from your laptop or the Apify “Run” button | **Low**        | Fine for dev, but the shared IP of the Apify build machine is still visible to Google.                            |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;If that’s all you’ll ever do, skip proxies and keep the code simpler. Otherwise, read on.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 2 . Why most production runs **should** use proxies&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| Phase                                        | Typical failure without proxy                                                    | Recommended proxy group                                                            |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| -------------------------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| **Stage A – RSS calls to `news.google.com`** | `429 Too many requests` or empty feed after \\~100–200 calls in a burst           | `GOOGLE_SERP` (rotation &amp; Google-safe) ([docs.apify.com][1])                       |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| **Stage B – Fetching article pages**         | Individual publishers return paywall / captcha after 5-30 rapid hits from one IP | `RESIDENTIAL` (safest) or a datacenter group if cost matters ([docs.apify.com][2]) |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* Apify’s own Google-News actors actually **require** a proxy input for this reason. ([Apify][3])&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* All Apify accounts (even free) already include the `GOOGLE_SERP`, `RESIDENTIAL`, and a shared datacenter pool, so you don’t need an extra provider. ([docs.apify.com][4])&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 3 . Minimal proxy config you can copy-paste&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```js&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;// Stage A – Google News RSS&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;const googleProxy = await Actor.createProxyConfiguration({&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    groups: ['GOOGLE_SERP'],          // rotation tuned for Google&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    countryCode: input.region || 'US' // honours user’s region choice&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;});&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;// Stage B – Article pages&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;const articleProxy = await Actor.createProxyConfiguration({&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    groups: ['RESIDENTIAL'],          // least likely to be blocked&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    countryCode: input.region || 'US'&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;});&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;```&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Pass `googleProxy` to the helper that fetches RSS, and `articleProxy` to the crawler that opens every publisher URL.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Don’t mix: `GOOGLE_SERP` only works for Google domains and will fail on `nytimes.com` or `bbc.co.uk`. ([docs.apify.com][1])&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 4 . Tuning tips&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| Tip                                                                                                                                     | Why / how                                                                |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| **Throttle** your RSS loop to \\~3 req/s and article crawler to \\~5 req/s per proxy IP.                                                  | Staying under human-like speed massively reduces bans.                   |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| **Tie proxies to sessions** (`useSessionPool: true`, `maxUsageCount: 5`).                                                               | Lets you reuse cookies while still rotating IPs before reputation tanks. |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| **Country targeting:** The proxy’s `countryCode`, not the Google News URL, determines the IP’s location. Leave it `US` as your default. |                                                                          |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| **Fallback strategy:** If an article 403s on datacenter proxy, retry once with `RESIDENTIAL`.                                           |                                                                          |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;| **Cost control:** Datacenter traffic is cheapest; start there and only promote domains that block you to residential.                   |                                                                          |&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;## 5 . What happens if you ignore proxies?&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* Google can start returning a truncated, “filtered for spam” feed, or simply respond with HTTP 429 after a few hundred unauthenticated hits from the same IP range.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* Popular publishers (e.g., *Washington Post*, *Bloomberg*) detect scraping patterns and either throw paywall splash pages or block the IP entirely.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* Your Apify actor might work in dev, then silently die during a bigger run and leave you with partial data.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;All of that costs more engineering time than a few proxy credits.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;### Bottom line&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* **Dev/QA:** feel free to run without a proxy.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;* **Anything you plan to schedule or share:** wire in `GOOGLE_SERP` for the Google step and a datacenter → residential fallback for article steps.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;That two-line `createProxyConfiguration` block is the cheapest insurance you can buy for a scraper.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;[1]: https://docs.apify.com/platform/proxy/google-serp-proxy \&quot;Google SERP proxy | Platform | Apify Documentation\&quot;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;[2]: https://docs.apify.com/platform/proxy \&quot;Proxy | Platform | Apify Documentation\&quot;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;[3]: https://apify.com/epctex/google-news-scraper \&quot;Google News Scraper · Apify\&quot;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;[4]: https://docs.apify.com/academy/expert-scraping-with-apify/solutions/rotating-proxies \&quot;VI - Rotating proxies/sessions | Academy | Apify Documentation\&quot;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;---------&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Can you create a task list based on that descriptions, taking into account Language - js + local setup, tests, deployement, etc etc etc - I need a fully working project as a result&quot;}]}]},&quot;mentioned_items&quot;:[],&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:0,&quot;content&quot;:&quot;Based on the blueprint, here's a comprehensive task list for creating a fully working Google News scraper Apify Actor:\n\n## Phase 1: Project Setup &amp; Structure\n\n### 1.1 Initialize Apify Project\n- [ ] Run `npx apify-cli create google-news-scraper`\n- [ ] Initialize git repository\n- [ ] Set up `.gitignore` for Node.js/Apify projects\n- [ ] Configure `package.json` with required dependencies\n\n### 1.2 Install Dependencies\n- [ ] Install core dependencies: `apify`, `unfluff`, `fast-xml-parser`\n- [ ] Install dev dependencies: `jest`, `@types/node`, `eslint`, `prettier`\n- [ ] Install optional: `playwright` for browser mode\n- [ ] Set up TypeScript configuration (optional but recommended)\n\n### 1.3 Project Structure\n- [ ] Create `src/` directory structure:\n  - `src/main.js` (entry point)\n  - `src/rss-fetcher.js` (Stage A logic)\n  - `src/article-crawler.js` (Stage B logic)\n  - `src/utils.js` (helper functions)\n  - `src/config.js` (constants and configurations)\n\n## Phase 2: Core Implementation\n\n### 2.1 Input Schema &amp; Configuration\n- [ ] Create `INPUT_SCHEMA.json` with all required fields\n- [ ] Implement input validation in `main.js`\n- [ ] Set up default values and error handling\n- [ ] Create configuration constants (timeouts, concurrency limits)\n\n### 2.2 Stage A - RSS Feed Processing\n- [ ] Implement `buildFeedUrl()` function with proper encoding\n- [ ] Create RSS fetcher with proxy support (`GOOGLE_SERP`)\n- [ ] Implement XML parsing with `fast-xml-parser`\n- [ ] Add deduplication logic using Map/Set\n- [ ] Implement date range slicing for large queries\n- [ ] Add progress tracking and resumption via KeyValueStore\n\n### 2.3 Stage B - Article Crawling\n- [ ] Implement URL extraction from Google News links\n- [ ] Set up RequestQueue population\n- [ ] Create CheerioCrawler with proper configuration\n- [ ] Implement Unfluff integration for content extraction\n- [ ] Add image collection and validation logic\n- [ ] Implement working image URL verification\n- [ ] Set up Dataset output with proper schema\n\n### 2.4 Proxy Integration\n- [ ] Configure `GOOGLE_SERP` proxy for RSS fetching\n- [ ] Configure `RESIDENTIAL`/datacenter proxy for article crawling\n- [ ] Implement session management\n- [ ] Add fallback proxy strategy\n- [ ] Implement rate limiting (3 req/s for RSS, 5 req/s for articles)\n\n## Phase 3: Testing &amp; Quality Assurance\n\n### 3.1 Unit Tests\n- [ ] Test RSS URL building with various parameters\n- [ ] Test XML parsing with mock RSS data\n- [ ] Test URL extraction from Google News links\n- [ ] Test content extraction with sample HTML\n- [ ] Test image validation logic\n- [ ] Test date range slicing logic\n\n### 3.2 Integration Tests\n- [ ] Test complete flow with small query (≤10 results)\n- [ ] Test with different regions/languages\n- [ ] Test date filtering functionality\n- [ ] Test browser mode vs. Cheerio mode\n- [ ] Test proxy rotation and fallback\n- [ ] Test error handling and retries\n\n### 3.3 Performance Tests\n- [ ] Test with `maxItems=1000` to check memory usage\n- [ ] Test concurrent crawling limits\n- [ ] Benchmark RSS fetching speed\n- [ ] Test timeout handling\n- [ ] Monitor proxy credit usage\n\n## Phase 4: Error Handling &amp; Robustness\n\n### 4.1 Error Handling\n- [ ] Implement comprehensive try-catch blocks\n- [ ] Add retry logic for failed requests\n- [ ] Handle malformed RSS feeds gracefully\n- [ ] Handle article parsing failures\n- [ ] Log failed URLs to KeyValueStore for audit\n- [ ] Implement graceful degradation for image validation\n\n### 4.2 Monitoring &amp; Logging\n- [ ] Add structured logging throughout the application\n- [ ] Implement progress reporting\n- [ ] Add performance metrics collection\n- [ ] Create debug mode with verbose logging\n- [ ] Add memory usage monitoring\n\n## Phase 5: Local Development &amp; Testing\n\n### 5.1 Local Setup\n- [ ] Create `.env` file with Apify token\n- [ ] Set up local Apify storage directories\n- [ ] Create sample input files for testing\n- [ ] Set up npm scripts for development\n- [ ] Configure ESLint and Prettier\n\n### 5.2 Development Tools\n- [ ] Create helper scripts for common tasks\n- [ ] Set up hot reload for development\n- [ ] Create mock data generators for testing\n- [ ] Set up debugging configuration\n- [ ] Create performance profiling setup\n\n## Phase 6: Documentation\n\n### 6.1 Code Documentation\n- [ ] Add JSDoc comments to all functions\n- [ ] Document input/output schemas\n- [ ] Create inline code comments for complex logic\n- [ ] Document proxy configuration options\n- [ ] Add troubleshooting guides in comments\n\n### 6.2 User Documentation\n- [ ] Create comprehensive README.md\n- [ ] Document all input parameters with examples\n- [ ] Create usage examples for different scenarios\n- [ ] Document output data structure\n- [ ] Add FAQ section for common issues\n\n## Phase 7: Deployment &amp; Publishing\n\n### 7.1 Pre-deployment Checklist\n- [ ] Run full test suite\n- [ ] Test with various input combinations\n- [ ] Verify proxy configurations work in Apify cloud\n- [ ] Check memory and CPU usage limits\n- [ ] Validate output data quality\n\n### 7.2 Apify Store Preparation\n- [ ] Create actor icon and screenshots\n- [ ] Write compelling actor description\n- [ ] Set appropriate pricing/credit usage\n- [ ] Configure actor categories and tags\n- [ ] Set up actor permissions (proxy usage)\n\n### 7.3 Deployment\n- [ ] Deploy to Apify platform using `apify push`\n- [ ] Test deployed version with sample runs\n- [ ] Set up monitoring and alerts\n- [ ] Create example runs for the store\n- [ ] Submit to Apify Store for review\n\n## Phase 8: Maintenance &amp; Extensions\n\n### 8.1 Monitoring &amp; Maintenance\n- [ ] Set up automated health checks\n- [ ] Monitor success rates and error patterns\n- [ ] Track proxy usage and costs\n- [ ] Monitor Google News RSS changes\n- [ ] Set up alerts for critical failures\n\n### 8.2 Optional Extensions\n- [ ] Add CSV/XLSX export functionality\n- [ ] Implement binary search for date ranges\n- [ ] Add support for topic-based feeds\n- [ ] Create webhook notifications\n- [ ] Add data deduplication across runs\n- [ ] Implement incremental crawling\n\n## Estimated Timeline\n- **Phase 1-2**: 3-4 days (core implementation)\n- **Phase 3**: 2-3 days (testing)\n- **Phase 4**: 1-2 days (error handling)\n- **Phase 5**: 1 day (local setup)\n- **Phase 6**: 1 day (documentation)\n- **Phase 7**: 1 day (deployment)\n- **Phase 8**: Ongoing (maintenance)\n\n**Total**: ~10-14 days for a production-ready actor\n\nWould you like me to start implementing any specific phase or create the initial project structure?\n&quot;},{&quot;id&quot;:2,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:3,&quot;content&quot;:&quot;main.py:0-513&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;text_node&quot;:{&quot;content&quot;:&quot;Below is a concise but complete blueprint for an **Apify Actor** that takes a single *search query* (plus an optional region/language override) and returns **title + URL + publication date + source + full text + list of working image URLs** for every Google News result it can reach.\n\n---\n\n## 0 . Why RSS + a two-stage crawl still works\n\n* Google News continues to expose every search or topic through RSS endpoints such as\n  `https://news.google.com/rss/search?q=&lt;QUERY&gt;&amp;hl=&lt;LANG&gt;&amp;gl=&lt;COUNTRY&gt;&amp;ceid=&lt;COUNTRY&gt;:&lt;LANG&gt;` ([newscatcherapi.com][1]).\n* Region/language are controlled by `hl`, `gl`, `ceid`; omit them and Google falls back to `en-US / US`([docs.feedly.com][2]).\n* RSS items carry only the headline &amp; link, so we must open each article ourselves to collect images and full text.\n\n---\n\n## 1 . Define the Actor’s input schema (`INPUT_SCHEMA.json`)\n\n```json\n{\n  \&quot;title\&quot;: \&quot;Google-News-Full-Scraper\&quot;,\n  \&quot;type\&quot;: \&quot;object\&quot;,\n  \&quot;properties\&quot;: {\n    \&quot;query\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;title\&quot;: \&quot;Google News search query\&quot; },\n    \&quot;region\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;default\&quot;: \&quot;US\&quot;, \&quot;title\&quot;: \&quot;Country code (gl)\&quot; },\n    \&quot;language\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;default\&quot;: \&quot;en-US\&quot;, \&quot;title\&quot;: \&quot;Language (hl)\&quot; },\n    \&quot;maxItems\&quot;: { \&quot;type\&quot;: \&quot;integer\&quot;, \&quot;default\&quot;: 0, \&quot;minimum\&quot;: 0, \&quot;title\&quot;: \&quot;0 = all available\&quot; },\n    \&quot;dateFrom\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;format\&quot;: \&quot;date\&quot;, \&quot;title\&quot;: \&quot;yyyy-MM-dd (optional)\&quot; },\n    \&quot;dateTo\&quot;:   { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;format\&quot;: \&quot;date\&quot;, \&quot;title\&quot;: \&quot;yyyy-MM-dd (optional)\&quot; },\n    \&quot;useBrowser\&quot;: { \&quot;type\&quot;: \&quot;boolean\&quot;, \&quot;default\&quot;: false, \&quot;title\&quot;: \&quot;Render JS with Playwright if needed\&quot; }\n  },\n  \&quot;required\&quot;: [\&quot;query\&quot;]\n}\n```\n\n---\n\n## 2 . Stage A – collect every RSS item\n\n| Step | What to do                                                                                                                                                                                                                                                                                                                                               | Key points                                                       |\n| ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |\n| A-1  | **Build the first feed URL** with the user’s `query`, `hl`, `gl`, `ceid`.                                                                                                                                                                                                                                                                                | Encode the query with `encodeURIComponent`.                      |\n| A-2  | **Fetch the feed** with `utils.requestAsBrowser({ url, method: 'GET', ... })`.                                                                                                                                                                                                                                                                           | Fast—no browser.                                                 |\n| A-3  | **Parse XML** via `fast-xml-parser`; push items into an in-memory Set keyed by GUID to de-dup. Each item gives us `{ title, link, pubDate, source }`.                                                                                                                                                                                                    | If `maxItems &gt; 0` stop when reached.                             |\n| A-4  | **If more items requested than Google’s \\~100-result cap**, slice the date range:&lt;br&gt;‣ Start with `dateTo` (or today) and walk backwards one day at a time.&lt;br&gt;‣ Append `before:` &amp; `after:` operators to the *query* (e.g. `\&quot;AI after:2025-05-10 before:2025-05-11\&quot;`).&lt;br&gt;‣ Repeat A-2 → A-3 until the Set size ≥ `maxItems` or we run out of days.&lt;br&gt; | One HTTP request per day is still faster than headless browsing. |\n\n&gt; **Tip:** store `lastDateChecked` in the Key-Value-Store so the actor can resume if it is restarted.\n\n---\n\n## 3 . Stage B – crawl every article URL\n\n### 3.1  Queue preparation\n\n* For each RSS item, extract the real URL:\n  *If the link already points off `news.google.com`, use it.*\n  *If it looks like `https://news.google.com/rss/articles/...&amp;url=&lt;ENCODED&gt;` → `decodeURIComponent(url)`.*\n* Push that URL into an **Apify RequestQueue** with `{ userData: { metaFromRssItem } }`.\n\n### 3.2  Pick the right crawler\n\n| If `useBrowser`   | Use                 | When                                              |\n| ----------------- | ------------------- | ------------------------------------------------- |\n| `false` (default) | `CheerioCrawler`    | 90 % of news sites deliver pure HTML.             |\n| `true`            | `PlaywrightCrawler` | For pay-walled or JS-rendered pages; higher cost. |\n\nCrawler options:\n\n```js\n{\n  maxConcurrency: 10,\n  maxRequestRetries: 2,\n  requestHandlerTimeoutSecs: 60\n}\n```\n\n### 3.3  In the `requestHandler`\n\n1. **Grab HTML** (`context.response.body`).\n2. **Parse** with **Unfluff** (Node) or **newspaper3k** (Python) to get:\n\n   * `title, text, author, date, image, videos, links, tags, lang`\n3. **Collect images**\n\n   * Combine `unfluff.image`, every `&lt;meta property=\&quot;og:image\&quot;&gt;`, and the first `&lt;article&gt; img`.\n   * **Deduplicate** the list.\n4. **Validate images**\n\n   ```js\n   const workingImages = [];\n   for (const imgUrl of images) {\n     try {\n       const { statusCode } = await utils.requestAsBrowser({ url: imgUrl, method: 'HEAD', timeoutSecs: 15, followRedirect: true });\n       if (statusCode &gt;= 200 &amp;&amp; statusCode &lt; 400) workingImages.push(imgUrl);\n     } catch { /* ignore */ }\n   }\n   ```\n5. **Push final record** to the default Dataset:\n\n   ```js\n   await Dataset.pushData({\n     query: input.query,\n     title,\n     url: page.url(),\n     source: meta.source,\n     publishedAt: meta.pubDate || date,\n     author,\n     text,\n     images: workingImages\n   });\n   ```\n\n---\n\n## 4 . Putting it together (`main.js` skeleton)\n\n```js\nimport { Actor, utils, Dataset, KeyValueStore, RequestQueue, CheerioCrawler } from 'apify';\nimport unfluff from 'unfluff';\nimport { parse } from 'fast-xml-parser';\n\nawait Actor.init();\nconst input = await Actor.getInput();\nconst { query, region = 'US', language = 'en-US', maxItems = 0, dateFrom, dateTo, useBrowser } = input;\n\n// ---------- Stage A ----------\nconst articles = new Map();   // guid → rssItem\n\nasync function fetchFeed(feedUrl) {\n  const { body } = await utils.requestAsBrowser({ url: feedUrl, timeoutSecs: 30 });\n  const xml = parse(body, { ignoreAttributes: false });\n  for (const item of xml.rss.channel.item ?? []) {\n    if (!articles.has(item.guid)) articles.set(item.guid, item);\n    if (maxItems &amp;&amp; articles.size &gt;= maxItems) return;\n  }\n}\n\nfunction buildFeed(q) {\n  const params = new URLSearchParams({\n    q: q,\n    hl: language,\n    gl: region,\n    ceid: `${region}:${language.split('-')[0]}`\n  });\n  return `https://news.google.com/rss/search?${params.toString()}`;\n}\n\n// Initial fetch or date slicing\nawait fetchFeed(buildFeed(query));\nif (maxItems &amp;&amp; articles.size &lt; maxItems) {\n  // walk back one day at a time between dateFrom / dateTo (or today)\n  // append after: / before: to query, call fetchFeed() repeatedly\n}\n\n// ---------- Stage B ----------\nconst rq = await RequestQueue.open();\nfor (const item of articles.values()) {\n  let url = item.link;\n  const u = new URL(url);\n  if (u.hostname === 'news.google.com' &amp;&amp; u.searchParams.has('url')) {\n    url = decodeURIComponent(u.searchParams.get('url'));\n  }\n  await rq.addRequest({ url, userData: item });\n}\n\nconst crawler = new CheerioCrawler({\n  requestQueue: rq,\n  maxConcurrency: 10,\n  requestHandler: async ({ request, $, body }) =&gt; {\n    const data = unfluff(body);\n    // ... image logic from step 3.3 ...\n    await Dataset.pushData(/* final record object */);\n  }\n});\nawait crawler.run();\n\nawait Actor.exit();\n```\n\nConvert to **TypeScript** if you want stricter typing; for **Python** replicate the same flow with `apify` + `aiohttp` + `python-dateutil` + `extruct`/`newspaper3k`.\n\n---\n\n## 5 . Testing &amp; deployment checklist\n\n1. `apify run` locally with a narrow query, no date slicing – verify dataset rows.\n2. Flip `useBrowser=true` on a JS-heavy site (e.g. *washingtonpost.com*) and ensure Playwright works.\n3. Stress-test with `maxItems=1000` – watch CPU &amp; memory; raise `APIFY_MEMORY_MBYTES` if needed.\n4. Set **Actor permissions**: allow `proxy = auto` only if you hit geo-based blocks.\n5. Push to the Apify Store; add *README* documenting the input fields and output schema.\n\n---\n\n### Extensions &amp; hardening (optional but recommended)\n\n* **Rate-limit Google**: add a short `await Actor.utils.sleep(200)` between RSS fetches.\n* **Robust date slicing**: binary-search the date range instead of daily stepping for very large queries.\n* **Retry/timeout hooks**: log failing article URLs to a separate Key-Value-Store key for audit.\n* **Unit tests**: mock an RSS XML and one article HTML to test parsing logic without network.\n* **Structured output**: emit a `.csv` or `.xlsx` in `key_value_stores/default/OUTPUT` for users who prefer spreadsheets.\n\n---\n\nWith this roadmap you can stand up a lean, reliable Google News → full-article scraper inside Apify in just a few hours, without dragging along the complexity of the full-featured actors you studied.\n\n[1]: https://www.newscatcherapi.com/blog/google-news-rss-search-parameters-the-missing-documentaiton?utm_source=chatgpt.com \&quot;Google News RSS Search Parameters: The Missing Docs\&quot;\n[2]: https://docs.feedly.com/article/375-what-are-some-of-the-advanced-keyword-alerts-google-news-search-parameters?utm_source=chatgpt.com \&quot;What are some of the advanced Google News search ...\&quot;\n\n\n### Short answer\n\n**Yes — use a proxy, at least for anything beyond quick, low-volume tests.**\nGoogle and many publisher sites rate-limit or geo-gate aggressive scrapers; rotating proxies keep you off their radar and let you choose the country that Google News should think you’re coming from. Below are the situations, choices and code snippets you’ll need.\n\n---\n\n## 1 . When you can get away **without** a proxy\n\n| Scenario                                                      | Risk of blocks | Notes                                                                                                             |\n| ------------------------------------------------------------- | -------------- | ----------------------------------------------------------------------------------------------------------------- |\n| ≤ a dozen RSS requests per hour and &lt; 100 article fetches     | **Low**        | Google News RSS is relatively tolerant, and most small/medium newsrooms don’t throttle HTML fetches that lightly. |\n| One-off debug runs from your laptop or the Apify “Run” button | **Low**        | Fine for dev, but the shared IP of the Apify build machine is still visible to Google.                            |\n\nIf that’s all you’ll ever do, skip proxies and keep the code simpler. Otherwise, read on.\n\n---\n\n## 2 . Why most production runs **should** use proxies\n\n| Phase                                        | Typical failure without proxy                                                    | Recommended proxy group                                                            |\n| -------------------------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n| **Stage A – RSS calls to `news.google.com`** | `429 Too many requests` or empty feed after \\~100–200 calls in a burst           | `GOOGLE_SERP` (rotation &amp; Google-safe) ([docs.apify.com][1])                       |\n| **Stage B – Fetching article pages**         | Individual publishers return paywall / captcha after 5-30 rapid hits from one IP | `RESIDENTIAL` (safest) or a datacenter group if cost matters ([docs.apify.com][2]) |\n\n* Apify’s own Google-News actors actually **require** a proxy input for this reason. ([Apify][3])\n* All Apify accounts (even free) already include the `GOOGLE_SERP`, `RESIDENTIAL`, and a shared datacenter pool, so you don’t need an extra provider. ([docs.apify.com][4])\n\n---\n\n## 3 . Minimal proxy config you can copy-paste\n\n```js\n// Stage A – Google News RSS\nconst googleProxy = await Actor.createProxyConfiguration({\n    groups: ['GOOGLE_SERP'],          // rotation tuned for Google\n    countryCode: input.region || 'US' // honours user’s region choice\n});\n\n// Stage B – Article pages\nconst articleProxy = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],          // least likely to be blocked\n    countryCode: input.region || 'US'\n});\n```\n\nPass `googleProxy` to the helper that fetches RSS, and `articleProxy` to the crawler that opens every publisher URL.\nDon’t mix: `GOOGLE_SERP` only works for Google domains and will fail on `nytimes.com` or `bbc.co.uk`. ([docs.apify.com][1])\n\n---\n\n## 4 . Tuning tips\n\n| Tip                                                                                                                                     | Why / how                                                                |\n| --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |\n| **Throttle** your RSS loop to \\~3 req/s and article crawler to \\~5 req/s per proxy IP.                                                  | Staying under human-like speed massively reduces bans.                   |\n| **Tie proxies to sessions** (`useSessionPool: true`, `maxUsageCount: 5`).                                                               | Lets you reuse cookies while still rotating IPs before reputation tanks. |\n| **Country targeting:** The proxy’s `countryCode`, not the Google News URL, determines the IP’s location. Leave it `US` as your default. |                                                                          |\n| **Fallback strategy:** If an article 403s on datacenter proxy, retry once with `RESIDENTIAL`.                                           |                                                                          |\n| **Cost control:** Datacenter traffic is cheapest; start there and only promote domains that block you to residential.                   |                                                                          |\n\n---\n\n## 5 . What happens if you ignore proxies?\n\n* Google can start returning a truncated, “filtered for spam” feed, or simply respond with HTTP 429 after a few hundred unauthenticated hits from the same IP range.\n* Popular publishers (e.g., *Washington Post*, *Bloomberg*) detect scraping patterns and either throw paywall splash pages or block the IP entirely.\n* Your Apify actor might work in dev, then silently die during a bigger run and leave you with partial data.\n\nAll of that costs more engineering time than a few proxy credits.\n\n---\n\n### Bottom line\n\n* **Dev/QA:** feel free to run without a proxy.\n* **Anything you plan to schedule or share:** wire in `GOOGLE_SERP` for the Google step and a datacenter → residential fallback for article steps.\n\nThat two-line `createProxyConfiguration` block is the cheapest insurance you can buy for a scraper.\n\n[1]: https://docs.apify.com/platform/proxy/google-serp-proxy \&quot;Google SERP proxy | Platform | Apify Documentation\&quot;\n[2]: https://docs.apify.com/platform/proxy \&quot;Proxy | Platform | Apify Documentation\&quot;\n[3]: https://apify.com/epctex/google-news-scraper \&quot;Google News Scraper · Apify\&quot;\n[4]: https://docs.apify.com/academy/expert-scraping-with-apify/solutions/rotating-proxies \&quot;VI - Rotating proxies/sessions | Academy | Apify Documentation\&quot;\n\n---------\nCan you create a task list based on that descriptions, taking into account Language - js + local setup, tests, deployement, etc etc etc - I need a fully working project as a result&quot;}},{&quot;id&quot;:1,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/yevheniimolodtsov/Documents/Development/personal/google-news-scrapper&quot;,&quot;folderRoot&quot;:&quot;/Users/yevheniimolodtsov/Documents/Development/personal/google-news-scrapper&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-08-04T15:52:34.782Z&quot;,&quot;response_text&quot;:&quot;Based on the blueprint, here's a comprehensive task list for creating a fully working Google News scraper Apify Actor:\n\n## Phase 1: Project Setup &amp; Structure\n\n### 1.1 Initialize Apify Project\n- [ ] Run `npx apify-cli create google-news-scraper`\n- [ ] Initialize git repository\n- [ ] Set up `.gitignore` for Node.js/Apify projects\n- [ ] Configure `package.json` with required dependencies\n\n### 1.2 Install Dependencies\n- [ ] Install core dependencies: `apify`, `unfluff`, `fast-xml-parser`\n- [ ] Install dev dependencies: `jest`, `@types/node`, `eslint`, `prettier`\n- [ ] Install optional: `playwright` for browser mode\n- [ ] Set up TypeScript configuration (optional but recommended)\n\n### 1.3 Project Structure\n- [ ] Create `src/` directory structure:\n  - `src/main.js` (entry point)\n  - `src/rss-fetcher.js` (Stage A logic)\n  - `src/article-crawler.js` (Stage B logic)\n  - `src/utils.js` (helper functions)\n  - `src/config.js` (constants and configurations)\n\n## Phase 2: Core Implementation\n\n### 2.1 Input Schema &amp; Configuration\n- [ ] Create `INPUT_SCHEMA.json` with all required fields\n- [ ] Implement input validation in `main.js`\n- [ ] Set up default values and error handling\n- [ ] Create configuration constants (timeouts, concurrency limits)\n\n### 2.2 Stage A - RSS Feed Processing\n- [ ] Implement `buildFeedUrl()` function with proper encoding\n- [ ] Create RSS fetcher with proxy support (`GOOGLE_SERP`)\n- [ ] Implement XML parsing with `fast-xml-parser`\n- [ ] Add deduplication logic using Map/Set\n- [ ] Implement date range slicing for large queries\n- [ ] Add progress tracking and resumption via KeyValueStore\n\n### 2.3 Stage B - Article Crawling\n- [ ] Implement URL extraction from Google News links\n- [ ] Set up RequestQueue population\n- [ ] Create CheerioCrawler with proper configuration\n- [ ] Implement Unfluff integration for content extraction\n- [ ] Add image collection and validation logic\n- [ ] Implement working image URL verification\n- [ ] Set up Dataset output with proper schema\n\n### 2.4 Proxy Integration\n- [ ] Configure `GOOGLE_SERP` proxy for RSS fetching\n- [ ] Configure `RESIDENTIAL`/datacenter proxy for article crawling\n- [ ] Implement session management\n- [ ] Add fallback proxy strategy\n- [ ] Implement rate limiting (3 req/s for RSS, 5 req/s for articles)\n\n## Phase 3: Testing &amp; Quality Assurance\n\n### 3.1 Unit Tests\n- [ ] Test RSS URL building with various parameters\n- [ ] Test XML parsing with mock RSS data\n- [ ] Test URL extraction from Google News links\n- [ ] Test content extraction with sample HTML\n- [ ] Test image validation logic\n- [ ] Test date range slicing logic\n\n### 3.2 Integration Tests\n- [ ] Test complete flow with small query (≤10 results)\n- [ ] Test with different regions/languages\n- [ ] Test date filtering functionality\n- [ ] Test browser mode vs. Cheerio mode\n- [ ] Test proxy rotation and fallback\n- [ ] Test error handling and retries\n\n### 3.3 Performance Tests\n- [ ] Test with `maxItems=1000` to check memory usage\n- [ ] Test concurrent crawling limits\n- [ ] Benchmark RSS fetching speed\n- [ ] Test timeout handling\n- [ ] Monitor proxy credit usage\n\n## Phase 4: Error Handling &amp; Robustness\n\n### 4.1 Error Handling\n- [ ] Implement comprehensive try-catch blocks\n- [ ] Add retry logic for failed requests\n- [ ] Handle malformed RSS feeds gracefully\n- [ ] Handle article parsing failures\n- [ ] Log failed URLs to KeyValueStore for audit\n- [ ] Implement graceful degradation for image validation\n\n### 4.2 Monitoring &amp; Logging\n- [ ] Add structured logging throughout the application\n- [ ] Implement progress reporting\n- [ ] Add performance metrics collection\n- [ ] Create debug mode with verbose logging\n- [ ] Add memory usage monitoring\n\n## Phase 5: Local Development &amp; Testing\n\n### 5.1 Local Setup\n- [ ] Create `.env` file with Apify token\n- [ ] Set up local Apify storage directories\n- [ ] Create sample input files for testing\n- [ ] Set up npm scripts for development\n- [ ] Configure ESLint and Prettier\n\n### 5.2 Development Tools\n- [ ] Create helper scripts for common tasks\n- [ ] Set up hot reload for development\n- [ ] Create mock data generators for testing\n- [ ] Set up debugging configuration\n- [ ] Create performance profiling setup\n\n## Phase 6: Documentation\n\n### 6.1 Code Documentation\n- [ ] Add JSDoc comments to all functions\n- [ ] Document input/output schemas\n- [ ] Create inline code comments for complex logic\n- [ ] Document proxy configuration options\n- [ ] Add troubleshooting guides in comments\n\n### 6.2 User Documentation\n- [ ] Create comprehensive README.md\n- [ ] Document all input parameters with examples\n- [ ] Create usage examples for different scenarios\n- [ ] Document output data structure\n- [ ] Add FAQ section for common issues\n\n## Phase 7: Deployment &amp; Publishing\n\n### 7.1 Pre-deployment Checklist\n- [ ] Run full test suite\n- [ ] Test with various input combinations\n- [ ] Verify proxy configurations work in Apify cloud\n- [ ] Check memory and CPU usage limits\n- [ ] Validate output data quality\n\n### 7.2 Apify Store Preparation\n- [ ] Create actor icon and screenshots\n- [ ] Write compelling actor description\n- [ ] Set appropriate pricing/credit usage\n- [ ] Configure actor categories and tags\n- [ ] Set up actor permissions (proxy usage)\n\n### 7.3 Deployment\n- [ ] Deploy to Apify platform using `apify push`\n- [ ] Test deployed version with sample runs\n- [ ] Set up monitoring and alerts\n- [ ] Create example runs for the store\n- [ ] Submit to Apify Store for review\n\n## Phase 8: Maintenance &amp; Extensions\n\n### 8.1 Monitoring &amp; Maintenance\n- [ ] Set up automated health checks\n- [ ] Monitor success rates and error patterns\n- [ ] Track proxy usage and costs\n- [ ] Monitor Google News RSS changes\n- [ ] Set up alerts for critical failures\n\n### 8.2 Optional Extensions\n- [ ] Add CSV/XLSX export functionality\n- [ ] Implement binary search for date ranges\n- [ ] Add support for topic-based feeds\n- [ ] Create webhook notifications\n- [ ] Add data deduplication across runs\n- [ ] Implement incremental crawling\n\n## Estimated Timeline\n- **Phase 1-2**: 3-4 days (core implementation)\n- **Phase 3**: 2-3 days (testing)\n- **Phase 4**: 1-2 days (error handling)\n- **Phase 5**: 1 day (local setup)\n- **Phase 6**: 1 day (documentation)\n- **Phase 7**: 1 day (deployment)\n- **Phase 8**: Ongoing (maintenance)\n\n**Total**: ~10-14 days for a production-ready actor\n\nWould you like me to start implementing any specific phase or create the initial project structure?\n&quot;,&quot;workspace_file_chunks&quot;:[{&quot;char_start&quot;:&quot;0&quot;,&quot;char_end&quot;:&quot;513&quot;,&quot;blob_name&quot;:&quot;a7dc19b6e5d761462685beb318f7a531675dee05977f6e687cbb62977e80fde0&quot;,&quot;file&quot;:{&quot;repoRoot&quot;:&quot;&quot;,&quot;pathName&quot;:&quot;main.py&quot;}}]}],&quot;feedbackStates&quot;:{&quot;temp-fe-91f527a4-1424-4469-a46d-ecf3a3fc102c&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;}},&quot;toolUseStates&quot;:{},&quot;draftExchange&quot;:{&quot;request_message&quot;:&quot;&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;}]},&quot;mentioned_items&quot;:[],&quot;status&quot;:&quot;draft&quot;},&quot;requestIds&quot;:[],&quot;isPinned&quot;:false,&quot;isShareable&quot;:true,&quot;extraData&quot;:{&quot;hasDirtyEdits&quot;:false},&quot;personaType&quot;:0,&quot;rootTaskUuid&quot;:&quot;14f687c7-071b-40a8-997c-23814bbf111a&quot;}},&quot;agentExecutionMode&quot;:&quot;manual&quot;,&quot;isPanelCollapsed&quot;:true,&quot;displayedAnnouncements&quot;:[]}" />
      </map>
    </option>
  </component>
</project>